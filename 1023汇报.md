# 1023汇报

## 目前干了什么
- 看代码梳理思路
    - 模型的设置：
        - 仅仅作为特征提取器还是带一个分类头（以在训练时评估）
        - 池化方式：平均池化 / CLS token
    - MemoryBank：让训练更高效，节省内存，主要因为方法中涉及到全局相似度矩阵，以及每隔几轮更新三元组数据

- 代码的细节
    - 预热策略，LANID通过参数设置来决定是否先经过几轮

- 改进的本质：挑选有价值的三元组


## 待解决的问题

- 三元组应该怎么选？肯定不可以仅仅通过阈值来判断。最简单的方式是抽样，或者最开始按照比例筛选

- 去掉调用LLM这一步，论文的创新点太少了，几乎退化到20年左右的论文内容量
- 关于LLM调用的置信度问题
- 考虑其他对比损失


## 现在的想法

### 思路1
- 分成两个训练阶段

    - (train-1)首先使用传统方法让模型有一个好的特征提取能力和判别能力 /(预热)

    - 前向传播计算相似度，该步骤采用在线挖掘的方式寻找难三元组，并认为模型的判断具有较高的可信度，筛选原则仍然是根据相似度得分。需要控制难三元组的数量
    - (train-2)得到候选三元组之后用LLM判断正类还是负类，进而使用三元组损失进一步优化模型，希望能进一步提高模型的特征提取能力。
    - (train-2,不考虑LLM)也可以用模型的输出作为伪标签

- 该方法采用在线挖掘，不需要计算全局相似度，这样肯定需要使用随机采样器

- 类似于cdac在使用半监督损失之外又加了一个KL散度损失微调阶段

### 思路2

### 思路2
- 由于现在三元组数量巨大，所以肯定需要2次筛选

    - 在筛选前就先得到一个高质量的子集，之后的筛选操作都在子集上进行

    - 按照原有思路得到初步筛选之后，进一步筛选，最简单的方式就是随机抽样。第一次筛选可根据比例控制数量
- 每隔几轮调用LLM，相当于离线生成三元组

- 只需要一个训练阶段，相当于两个损失联合训练
- 使用全局相似度


