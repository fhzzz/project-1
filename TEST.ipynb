{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49fefd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True, False, False]]) \n",
      " tensor([[ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False]])\n",
      "tensor([[ 1.,  0., -1., -1.],\n",
      "        [ 0.,  1., -1., -1.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "labels = torch.tensor([1, 2, -1, -1])\n",
    "sim = torch.rand(4, 4)\n",
    "global_R = torch.full((labels.size(0), labels.size(0)), -1.0)\n",
    "mask_label = (labels != -1)\n",
    "label_mask = mask_label.unsqueeze(0) & mask_label.unsqueeze(1)\n",
    "label_R = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n",
    "global_R[label_mask] = label_R[label_mask]\n",
    "\n",
    "print(mask_label.unsqueeze(0), '\\n', mask_label.unsqueeze(1))\n",
    "print(global_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c33dc44",
   "metadata": {},
   "source": [
    "从上面的输出结果可以看到，`.unsqueeze()`是在指定参数上增加维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a541a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4763, 0.6616, 0.2477, 0.4547],\n",
      "        [0.0404, 0.4367, 0.2876, 0.0377],\n",
      "        [0.7667, 0.4985, 0.9509, 0.6440],\n",
      "        [0.2946, 0.5130, 0.3965, 0.5472]])\n",
      "tensor([[ 1.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [-1.,  0.,  1., -1.],\n",
      "        [ 0., -1.,  0., -1.]])\n"
     ]
    }
   ],
   "source": [
    "other_mask = ~label_mask\n",
    "l, u = 0.5, 0.9\n",
    "global_R[other_mask & (sim >= u)] = torch.tensor(1.0)\n",
    "global_R[other_mask & (sim <= l)] = torch.tensor(0.0)\n",
    "print(sim)\n",
    "print(global_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "085cbc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 0], [3, 1]]\n"
     ]
    }
   ],
   "source": [
    "uncert_mask = (global_R == -1)\n",
    "mask = torch.tril(uncert_mask, diagonal=-1)\n",
    "row, col = torch.where(mask)\n",
    "uncert_ij = torch.stack([row, col], dim=1).tolist()\n",
    "print(uncert_ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0ee582b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7474)\n"
     ]
    }
   ],
   "source": [
    "# 计算损失\n",
    "pos_mask = (global_R == 1)\n",
    "neg_mask = (global_R == 0)\n",
    "eps = 1e-10\n",
    "pos_entropy = -torch.log(torch.clamp(sim, eps, 1.0)) * pos_mask\n",
    "neg_entropy = -torch.log(torch.clamp(1 - sim, eps, 1.0)) * neg_mask\n",
    "loss = pos_entropy.mean() + neg_entropy.mean() + u - l\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b671cc8",
   "metadata": {},
   "source": [
    "## 第1次train梳理\n",
    "\n",
    "- 矩阵R没有办法拼接，选不确定性样本对必须全局来弄。\n",
    "- 也就是说，得先得到全局的feats，然后全局计算相似度，主要的开销是这里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfd2963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange, tqdm\n",
    "def train(self, args):\n",
    "\n",
    "    for epoch in trange(args.train_epochs, desc=\"Training\"):\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        for batch in tqdm(train_semi_dataloader):\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # input_ids, attention_mask, label\n",
    "                feats_batch = model()\n",
    "\n",
    "                sim_batch = self.get_sim_score(feats_batch)\n",
    "                R_batch = self.get_R(sim_batch, u, l)\n",
    "\n",
    "                # 计算相似度损失\n",
    "                sim_loss = pos_entropy + neg_entropy\n",
    "\n",
    "                indices_pairs_batch = self.get_uncert_pairs(R_batch)\n",
    "                text_pairs_batch = self.get_text_pairs(indices_pairs_batch)\n",
    "\n",
    "                global_R = torch.cat((global_R, R_batch))\n",
    "                indices_pairs = torch.cat((indices_pairs, indices_pairs_batch))\n",
    "                text_pairs = torch.cat((text_pairs, text_pairs_batch))\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            # 调用LLM: dict{pair_index, llm_pred, conf}\n",
    "            llm_generated_outputs = self.llm_labeling(text_pairs)\n",
    "            # 更新R\n",
    "            new_R = self.update_R(llm_generated_outputs, global_R)\n",
    "            # 更新数据集，第一轮应该是构建数据集\n",
    "            # triplet_dataset: (anchor, pos, neg)\n",
    "            # 如果全部这样选负例，会不会导致一开始训练太难了？\n",
    "            # 因为FaceNet提到了这个问题，LANID则直接随机选取负例\n",
    "            triplet_dataset = self.update_data(llm_generated_outputs)\n",
    "            triplet_dataloader = DataLoader(triplet_dataset)\n",
    "\n",
    "            for batch in triplet_dataloader:\n",
    "                seq_emb = model(batch)\n",
    "\n",
    "                tri_loss = self.tri_loss(anchor, pos, neg)\n",
    "\n",
    "            tri_loss += tri_loss\n",
    "\n",
    "        loss = sim_loss + tri_loss\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daf8b8f",
   "metadata": {},
   "source": [
    "## 第2次train梳理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fae0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, args, ):\n",
    "\n",
    "    for epoch in trange(args.num_train_epochs, desc=\"Training\"):\n",
    "        # 1. 相似度损失\n",
    "        for batch in tqdm(train_semi_dataloader):\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # input_ids, attention_mask, label\n",
    "                feats_batch = model()\n",
    "\n",
    "                sim_batch = self.get_sim_score(feats_batch)\n",
    "                R_batch = self.get_R(sim_batch, u, l)\n",
    "\n",
    "                # 计算相似度损失\n",
    "                sim_loss = pos_entropy + neg_entropy\n",
    "            sim_loss += sim_loss\n",
    "        \n",
    "        # 2. 三元组损失（每隔几轮更新）\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            # 调用LLM标注，得到标注结果\n",
    "            # 这里llm_labeling方法中已经包含前期步骤：先拿到**全局**特征/sim/R\n",
    "            # feats, y_true = self.eval(train_semi_dataloader)\n",
    "            # sim_mat = self.get_sim_score(feats)\n",
    "            # global_R = self.get_global_R(sim_mat, u, l)\n",
    "            # indices_pairs = self.get_uncert_pairs(global_R)\n",
    "            # text_pairs = self.get_text_pairs(indices_pairs)\n",
    "            llm_outputs = self.llm_labeling(text_pairs)\n",
    "\n",
    "            # 更新R矩阵和三元组数据集\n",
    "            global_R = self.update_R(global_R)\n",
    "            tri_dataset = self.update_data(llm_outputs)\n",
    "            \n",
    "        for batch in tri_dataloader:\n",
    "            anchor, neg, pos = model()\n",
    "\n",
    "            tri_loss = self.tri_loss(anchor, pos, neg)\n",
    "\n",
    "        loss = sim_loss + tri_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d361ef",
   "metadata": {},
   "source": [
    "## 关于模型\n",
    "\n",
    "### CDAC\n",
    "- backbone + [activate + dropout + classify]\n",
    "- mean pooling\n",
    "\n",
    "### ALUP\n",
    "- feature_ext: \n",
    "    - backbone + dropout\n",
    "    - 没有维度变化，默认768\n",
    "\n",
    "- simple_forward(对比学习投影层): \n",
    "    - backbone + dropout + head\n",
    "    - head: linear + relu + dropout + linear\n",
    "    - head_feat_dim\n",
    "- CLS token\n",
    "\n",
    "### LANID\n",
    "- 特征提取器\n",
    "- CLS token: cls_emb = output_hidden[-1][:, 0]\n",
    "- backbone + normalize(head(cls_emb))\n",
    "- head: linear + relu + dropout + linear\n",
    "- head_feat_dim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56335df",
   "metadata": {},
   "source": [
    "## 第3次train梳理(1026)\n",
    "\n",
    "- 得到伪标签\n",
    "    - 模型前向传播得到特征向量，\n",
    "    - 进行聚类\n",
    "    - 顺带得到聚类评估结果\n",
    "\n",
    "- 更新数据集，得到 *train_dataloader*\n",
    "\n",
    "- 正常训练流程\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab04fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data(self, indices_pairs, ):\n",
    "\n",
    "    feats, _ = self.get_features(args, train_semi_dataloader)\n",
    "    km = KMeans(n_clusters=self.num_labels).fit(feats)\n",
    "    cluster_centroids, y_pred = km.cluster_centers_, km.labels_\n",
    "    # 匈牙利算法进行对齐\n",
    "    cluster_centroids, y_pred = self.alignment(self.centroids, cluster_centroids, y_pred)\n",
    "    self.centroids = cluster_centroids\n",
    "    # 匈牙利算法将预测结果映射到真实标签：\n",
    "    # y_pred_map: 每个具体样本预测标签对应映射后的标签，\n",
    "    # cluster_map: 每个聚类中心对应映射后的标签\n",
    "    y_pred_map, cluster_map, cluster_map_opp = self.get_hungray_aligment(y_pred, y_true)\n",
    "\n",
    "    relations = []\n",
    "    for step, (i, j) in enumerate(indices_pairs):\n",
    "        i_label = y_pred_map[indices_pairs[step][0]]\n",
    "        j_label = y_pred_map[indices_pairs[step][1]]\n",
    "        if i_label == j_label:\n",
    "            relations[\"\"]\n",
    "\n",
    "        \n",
    "def alignment(self, old_centroids, new_centroids, cluster_labels):\n",
    "    self.logger.info(\"***** Conducting Alignment *****\")\n",
    "    if old_centroids is not None:\n",
    "\n",
    "        old_centroids = old_centroids\n",
    "        new_centroids = new_centroids\n",
    "        \n",
    "        DistanceMatrix = np.linalg.norm(old_centroids[:,np.newaxis,:]-new_centroids[np.newaxis,:,:],axis=2) \n",
    "        row_ind, col_ind = linear_sum_assignment(DistanceMatrix)\n",
    "        \n",
    "        aligned_centroids = np.zeros_like(old_centroids)\n",
    "        alignment_labels = list(col_ind)\n",
    "\n",
    "        for i in range(self.num_labels):\n",
    "            label = alignment_labels[i]\n",
    "            aligned_centroids[i] = new_centroids[label]\n",
    "        # 新label对应老label\n",
    "        pseudo2label = {label:i for i,label in enumerate(alignment_labels)}\n",
    "        pseudo_labels = np.array([pseudo2label[label] for label in cluster_labels])\n",
    "\n",
    "    else:\n",
    "        aligned_centroids = new_centroids    \n",
    "        pseudo_labels = cluster_labels \n",
    "\n",
    "    self.logger.info(\"***** Update Pseudo Labels With Real Labels *****\")\n",
    "    \n",
    "    return aligned_centroids, pseudo_labels\n",
    "\n",
    "\n",
    "def get_hungray_aligment(self, y_pred, y_true):\n",
    "    num_test_samples = len(y_pred)\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D))\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    ind = np.transpose(np.asarray(linear_sum_assignment(w.max() - w)))\n",
    "    y_pred_map = []\n",
    "    cluster_map = [0]*len(ind)\n",
    "    cluster_map_opp = [0]*len(ind)\n",
    "    for i in range(num_test_samples):\n",
    "        yp = y_pred[i]\n",
    "        y_pred_map.append(ind[yp][1])\n",
    "    y_pred_map = np.asarray(y_pred_map)\n",
    "\n",
    "    for item in ind:\n",
    "        cluster_map[item[0]] = item[1]\n",
    "        cluster_map_opp[item[1]] = item[0]\n",
    "    cluster_map = np.asarray(cluster_map)\n",
    "    cluster_map_opp = np.asarray(cluster_map_opp)\n",
    "    assert np.all(cluster_map[cluster_map_opp] == np.arange(len(ind)))\n",
    "    return y_pred_map, cluster_map, cluster_map_opp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
