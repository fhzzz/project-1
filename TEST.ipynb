{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49fefd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True, False, False]]) \n",
      " tensor([[ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False]])\n",
      "tensor([[ 1.,  0., -1., -1.],\n",
      "        [ 0.,  1., -1., -1.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "labels = torch.tensor([1, 2, -1, -1])\n",
    "sim = torch.rand(4, 4)\n",
    "global_R = torch.full((labels.size(0), labels.size(0)), -1.0)\n",
    "mask_label = (labels != -1)\n",
    "label_mask = mask_label.unsqueeze(0) & mask_label.unsqueeze(1)\n",
    "label_R = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n",
    "global_R[label_mask] = label_R[label_mask]\n",
    "\n",
    "print(mask_label.unsqueeze(0), '\\n', mask_label.unsqueeze(1))\n",
    "print(global_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c33dc44",
   "metadata": {},
   "source": [
    "从上面的输出结果可以看到，`.unsqueeze()`是在指定参数上增加维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a541a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4763, 0.6616, 0.2477, 0.4547],\n",
      "        [0.0404, 0.4367, 0.2876, 0.0377],\n",
      "        [0.7667, 0.4985, 0.9509, 0.6440],\n",
      "        [0.2946, 0.5130, 0.3965, 0.5472]])\n",
      "tensor([[ 1.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [-1.,  0.,  1., -1.],\n",
      "        [ 0., -1.,  0., -1.]])\n"
     ]
    }
   ],
   "source": [
    "other_mask = ~label_mask\n",
    "l, u = 0.5, 0.9\n",
    "global_R[other_mask & (sim >= u)] = torch.tensor(1.0)\n",
    "global_R[other_mask & (sim <= l)] = torch.tensor(0.0)\n",
    "print(sim)\n",
    "print(global_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "085cbc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 0], [3, 1]]\n"
     ]
    }
   ],
   "source": [
    "uncert_mask = (global_R == -1)\n",
    "mask = torch.tril(uncert_mask, diagonal=-1)\n",
    "row, col = torch.where(mask)\n",
    "uncert_ij = torch.stack([row, col], dim=1).tolist()\n",
    "print(uncert_ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0ee582b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7474)\n"
     ]
    }
   ],
   "source": [
    "# 计算损失\n",
    "pos_mask = (global_R == 1)\n",
    "neg_mask = (global_R == 0)\n",
    "eps = 1e-10\n",
    "pos_entropy = -torch.log(torch.clamp(sim, eps, 1.0)) * pos_mask\n",
    "neg_entropy = -torch.log(torch.clamp(1 - sim, eps, 1.0)) * neg_mask\n",
    "loss = pos_entropy.mean() + neg_entropy.mean() + u - l\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b671cc8",
   "metadata": {},
   "source": [
    "## 第1次train梳理\n",
    "\n",
    "- 矩阵R没有办法拼接，选不确定性样本对必须全局来弄。\n",
    "- 也就是说，得先得到全局的feats，然后全局计算相似度，主要的开销是这里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfd2963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange, tqdm\n",
    "def train(self, args):\n",
    "\n",
    "    for epoch in trange(args.train_epochs, desc=\"Training\"):\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        for batch in tqdm(train_semi_dataloader):\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # input_ids, attention_mask, label\n",
    "                feats_batch = model()\n",
    "\n",
    "                sim_batch = self.get_sim_score(feats_batch)\n",
    "                R_batch = self.get_R(sim_batch, u, l)\n",
    "\n",
    "                # 计算相似度损失\n",
    "                sim_loss = pos_entropy + neg_entropy\n",
    "\n",
    "                indices_pairs_batch = self.get_uncert_pairs(R_batch)\n",
    "                text_pairs_batch = self.get_text_pairs(indices_pairs_batch)\n",
    "\n",
    "                global_R = torch.cat((global_R, R_batch))\n",
    "                indices_pairs = torch.cat((indices_pairs, indices_pairs_batch))\n",
    "                text_pairs = torch.cat((text_pairs, text_pairs_batch))\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            # 调用LLM: dict{pair_index, llm_pred, conf}\n",
    "            llm_generated_outputs = self.llm_labeling(text_pairs)\n",
    "            # 更新R\n",
    "            new_R = self.update_R(llm_generated_outputs, global_R)\n",
    "            # 更新数据集，第一轮应该是构建数据集\n",
    "            # triplet_dataset: (anchor, pos, neg)\n",
    "            # 如果全部这样选负例，会不会导致一开始训练太难了？\n",
    "            # 因为FaceNet提到了这个问题，LANID则直接随机选取负例\n",
    "            triplet_dataset = self.update_data(llm_generated_outputs)\n",
    "            triplet_dataloader = DataLoader(triplet_dataset)\n",
    "\n",
    "            for batch in triplet_dataloader:\n",
    "                seq_emb = model(batch)\n",
    "\n",
    "                tri_loss = self.tri_loss(anchor, pos, neg)\n",
    "\n",
    "            tri_loss += tri_loss\n",
    "\n",
    "        loss = sim_loss + tri_loss\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daf8b8f",
   "metadata": {},
   "source": [
    "## 第2次train梳理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fae0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, args, ):\n",
    "\n",
    "    for epoch in trange(args.num_train_epochs, desc=\"Training\"):\n",
    "        # 1. 相似度损失\n",
    "        for batch in tqdm(train_semi_dataloader):\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # input_ids, attention_mask, label\n",
    "                feats_batch = model()\n",
    "\n",
    "                sim_batch = self.get_sim_score(feats_batch)\n",
    "                R_batch = self.get_R(sim_batch, u, l)\n",
    "\n",
    "                # 计算相似度损失\n",
    "                sim_loss = pos_entropy + neg_entropy\n",
    "            sim_loss += sim_loss\n",
    "        \n",
    "        # 2. 三元组损失（每隔几轮更新）\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            # 调用LLM标注，得到标注结果\n",
    "            # 这里llm_labeling方法中已经包含前期步骤：先拿到**全局**特征/sim/R\n",
    "            # feats, y_true = self.eval(train_semi_dataloader)\n",
    "            # sim_mat = self.get_sim_score(feats)\n",
    "            # global_R = self.get_global_R(sim_mat, u, l)\n",
    "            # indices_pairs = self.get_uncert_pairs(global_R)\n",
    "            # text_pairs = self.get_text_pairs(indices_pairs)\n",
    "            llm_outputs = self.llm_labeling(text_pairs)\n",
    "\n",
    "            # 更新R矩阵和三元组数据集\n",
    "            global_R = self.update_R(global_R)\n",
    "            tri_dataset = self.update_data(llm_outputs)\n",
    "            \n",
    "        for batch in tri_dataloader:\n",
    "            anchor, neg, pos = model()\n",
    "\n",
    "            tri_loss = self.tri_loss(anchor, pos, neg)\n",
    "\n",
    "        loss = sim_loss + tri_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
